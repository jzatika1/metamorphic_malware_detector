import logging
from typing import List, Tuple, Dict, Union
from multiprocessing import Pool, cpu_count
from utils.utils import convert_opcodes_to_numeric
from utils.sequence_operations import extract_opcode_sequences

def process_files_in_batches(file_paths: List[str], batch_size: int, remove_duplicates: bool = True, debug: bool = False) -> Tuple[List[List[Union[bytes, int]]], List[List[int]], Dict[Union[bytes, int], int], Dict[str, List[int]]]:
    """
    Process files in batches to extract opcode sequences and convert them to numeric sequences.
    
    Args:
        file_paths (List[str]): List of file paths to process.
        batch_size (int): Number of files to process in each batch.
        remove_duplicates (bool): If True, remove duplicate sequences (for training). If False, keep all sequences (for classification).
        debug (bool): If True, enable debug logging.
    
    Returns:
        Tuple[List[List[Union[bytes, int]]], List[List[int]], Dict[Union[bytes, int], int], Dict[str, List[int]]]: 
            - List of opcode sequences
            - List of numeric sequences
            - Opcode to numeric mapping
            - Dictionary mapping file paths to lists of their sequence lengths
    """
    all_opcode_sequences = []
    opcode_map = {}
    unique_sequences_set = set()
    duplicate_count = 0
    file_sequence_lengths = {}  # New dictionary to store sequence lengths for each file

    with Pool(processes=cpu_count()) as pool:
        for i in range(0, len(file_paths), batch_size):
            batch_files = file_paths[i:i + batch_size]
            opcode_sequences_batches = pool.map(extract_opcode_sequences, batch_files)
            
            for file, opcodes in zip(batch_files, opcode_sequences_batches):
                if debug:
                    logging.debug(f"Extracted opcode sequences for {file}: {opcodes[:5]}...")  # Log only first 5 sequences
                
                file_lengths = []  # List to store sequence lengths for this file
                if remove_duplicates:
                    for seq in opcodes:
                        seq_tuple = tuple(seq)
                        if seq_tuple not in unique_sequences_set:
                            unique_sequences_set.add(seq_tuple)
                            all_opcode_sequences.append(seq)
                            file_lengths.append(len(seq))
                        else:
                            duplicate_count += 1
                else:
                    all_opcode_sequences.extend(opcodes)
                    file_lengths = [len(seq) for seq in opcodes]
                
                file_sequence_lengths[file] = file_lengths

            # Update opcode_map
            for seq in all_opcode_sequences:
                for op in seq:
                    if op not in opcode_map:
                        opcode_map[op] = len(opcode_map)

        # Convert to numeric sequences
        all_numeric_sequences = pool.starmap(
            convert_opcodes_to_numeric, 
            [(all_opcode_sequences, opcode_map)]
        )[0]  # We only have one result, so we take the first element

    if debug:
        logging.debug(f"Number of unique opcode sequences: {len(all_opcode_sequences)}")
        logging.debug(f"Number of unique opcodes: {len(opcode_map)}")
        logging.debug(f"Number of files processed: {len(file_sequence_lengths)}")
    
    if remove_duplicates:
        logging.info(f"Total duplicates removed: {duplicate_count}")
    else:
        logging.info("No duplicates removed (classification mode)")

    return all_opcode_sequences, all_numeric_sequences, opcode_map, file_sequence_lengths