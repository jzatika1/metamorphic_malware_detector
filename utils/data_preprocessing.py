from multiprocessing import Pool, cpu_count
from utils.utils import convert_opcodes_to_numeric
from utils.sequence_operations import extract_opcode_sequences
from utils.logger import Logger

def process_files_in_batches(file_paths, batch_size, remove_duplicates=True, logger=None):
    """
    Process files in batches to extract opcode sequences and convert them to numeric sequences.
    
    Args:
        file_paths: List of file paths to process.
        batch_size: Number of files to process in each batch.
        remove_duplicates: If True, remove duplicate sequences (for training). If False, keep all sequences (for classification).
        logger: Logger instance. If not provided, a new one will be created.
    
    Returns:
        Tuple: 
            - List of opcode sequences
            - List of numeric sequences
            - Opcode to numeric mapping
            - Dictionary mapping file paths to lists of their sequence lengths
    """
    if logger is None:
        logger = Logger.setup('data_preprocessing')

    logger.info(f"Starting to process {len(file_paths)} files in batches of {batch_size}")
    logger.info(f"Duplicate removal is {'enabled' if remove_duplicates else 'disabled'}")

    all_opcode_sequences = []
    opcode_map = {}
    unique_sequences_set = set()
    duplicate_count = 0
    file_sequence_lengths = {}

    with Pool(processes=cpu_count()) as pool:
        logger.info(f"Using a pool of {cpu_count()} processes")

        for i in range(0, len(file_paths), batch_size):
            batch_files = file_paths[i:i + batch_size]
            logger.info(f"Processing batch {i//batch_size + 1} with {len(batch_files)} files")

            opcode_sequences_batches = pool.map(extract_opcode_sequences, batch_files)
            
            for file, opcodes in zip(batch_files, opcode_sequences_batches):
                file_lengths = []
                if remove_duplicates:
                    for seq in opcodes:
                        seq_tuple = tuple(seq)
                        if seq_tuple not in unique_sequences_set:
                            unique_sequences_set.add(seq_tuple)
                            all_opcode_sequences.append(seq)
                            file_lengths.append(len(seq))
                        else:
                            duplicate_count += 1
                else:
                    all_opcode_sequences.extend(opcodes)
                    file_lengths = [len(seq) for seq in opcodes]
                
                file_sequence_lengths[file] = file_lengths
                Logger.debug(logger, f"Processed file: {file}, extracted {len(file_lengths)} sequences")

            # Update opcode_map
            for seq in all_opcode_sequences:
                for op in seq:
                    if op not in opcode_map:
                        opcode_map[op] = len(opcode_map)
            
            logger.info(f"Batch {i//batch_size + 1} processed. Total sequences so far: {len(all_opcode_sequences)}")

        logger.info("Converting opcode sequences to numeric sequences")
        all_numeric_sequences = pool.starmap(
            convert_opcodes_to_numeric, 
            [(all_opcode_sequences, opcode_map)]
        )[0]  # We only have one result, so we take the first element

    logger.info(f"Processing completed. Total opcode sequences: {len(all_opcode_sequences)}")
    logger.info(f"Total numeric sequences: {len(all_numeric_sequences)}")
    logger.info(f"Total unique opcodes: {len(opcode_map)}")
    if remove_duplicates:
        logger.info(f"Duplicates removed: {duplicate_count}")

    Logger.debug(logger, f"Opcode map: {opcode_map}")
    Logger.debug(logger, f"File sequence lengths: {file_sequence_lengths}")

    return all_opcode_sequences, all_numeric_sequences, opcode_map, file_sequence_lengths